{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "from src.preprocessing import read, tokenize, preprocess\n",
    "from src.helpers import process_query_results\n",
    "from src.database import Database, PositionalDatabase\n",
    "from src.query import query_database, pos_query_database\n",
    "from src.evaluation import map_at_k, mar_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"/Users/stighellemans/Desktop/Information_Retrieval/assignments/assignment 1\")\n",
    "base_small_doc_path = base_path / \"full_docs_small\"\n",
    "base_large_doc_path = base_path / \"full_docs\"\n",
    "\n",
    "small_docs = {int(re.search(r'\\d+', doc_path.name).group()): doc_path for doc_path in base_small_doc_path.glob(\"*.txt\")}\n",
    "large_docs = {int(re.search(r'\\d+', doc_path.name).group()): doc_path for doc_path in base_large_doc_path.glob(\"*.txt\")}\n",
    "\n",
    "small_queries = pd.read_csv(base_path / \"dev_small_queries - dev_small_queries.csv\", index_col=\"Query number\").to_dict()[\"Query\"]\n",
    "small_query_results = pd.read_csv(base_path / \"dev_query_results_small.csv\", index_col=\"Query_number\")\n",
    "small_query_results = process_query_results(small_queries, small_query_results)\n",
    "\n",
    "large_queries = pd.read_csv(base_path / \"dev_queries.tsv\", delimiter=\"\\t\", index_col=\"Query number\").to_dict()[\"Query\"]\n",
    "large_query_results = pd.read_csv(base_path / \"dev_query_results.csv\", index_col=\"Query_number\")\n",
    "large_query_results = process_query_results(large_queries, large_query_results)\n",
    "\n",
    "test_queries = pd.read_csv(base_path / \"queries.csv\", delimiter=\"\\t\", index_col=\"Query number\").to_dict()[\"Query\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_subset = {ID: large_docs[ID] for ID in list(large_docs.keys())[90000:190000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(read(doc)) for doc in large_subset.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 0: 100%|██████████| 5000/5000 [01:49<00:00, 45.76it/s]\n",
      "Processing Batch 1: 100%|██████████| 5000/5000 [01:50<00:00, 45.22it/s]\n",
      "Processing Batch 2: 100%|██████████| 5000/5000 [01:47<00:00, 46.70it/s]\n",
      "Processing Batch 3: 100%|██████████| 5000/5000 [01:51<00:00, 45.03it/s]\n",
      "Processing Batch 4: 100%|██████████| 5000/5000 [01:48<00:00, 46.03it/s]\n",
      "Processing Batch 5: 100%|██████████| 5000/5000 [01:50<00:00, 45.14it/s]\n",
      "Processing Batch 6: 100%|██████████| 5000/5000 [01:50<00:00, 45.34it/s]\n",
      "Processing Batch 7: 100%|██████████| 5000/5000 [01:52<00:00, 44.51it/s]\n",
      "Processing Batch 8: 100%|██████████| 5000/5000 [01:56<00:00, 42.88it/s]\n",
      "Processing Batch 9: 100%|██████████| 5000/5000 [01:57<00:00, 42.73it/s]\n",
      "Processing Batch 10: 100%|██████████| 5000/5000 [01:52<00:00, 44.41it/s]\n",
      "Processing Batch 11: 100%|██████████| 5000/5000 [01:55<00:00, 43.41it/s]\n",
      "Processing Batch 12: 100%|██████████| 5000/5000 [01:59<00:00, 41.91it/s]\n",
      "Processing Batch 13: 100%|██████████| 5000/5000 [01:54<00:00, 43.67it/s]\n",
      "Processing Batch 14: 100%|██████████| 5000/5000 [01:50<00:00, 45.13it/s]\n",
      "Processing Batch 15: 100%|██████████| 5000/5000 [01:49<00:00, 45.79it/s]\n",
      "Processing Batch 16: 100%|██████████| 5000/5000 [01:55<00:00, 43.27it/s]\n",
      "Processing Batch 17: 100%|██████████| 5000/5000 [02:04<00:00, 40.07it/s]\n",
      "Processing Batch 18: 100%|██████████| 5000/5000 [01:59<00:00, 41.89it/s]\n",
      "Processing Batch 19: 100%|██████████| 5000/5000 [01:46<00:00, 47.16it/s]\n",
      "Processing Batch 20: 100%|██████████| 5000/5000 [02:02<00:00, 40.94it/s]\n",
      "Processing Batch 21: 100%|██████████| 5000/5000 [01:53<00:00, 43.92it/s]\n",
      "Processing Batch 22: 100%|██████████| 5000/5000 [01:55<00:00, 43.38it/s]\n",
      "Processing Batch 23: 100%|██████████| 5000/5000 [01:58<00:00, 42.16it/s]\n",
      "Processing Batch 24: 100%|██████████| 5000/5000 [01:53<00:00, 44.09it/s]\n",
      "Processing Batch 25: 100%|██████████| 5000/5000 [01:51<00:00, 44.66it/s]\n",
      "Processing Batch 26: 100%|██████████| 5000/5000 [01:57<00:00, 42.41it/s]\n",
      "Processing Batch 27: 100%|██████████| 5000/5000 [01:59<00:00, 41.73it/s]\n",
      "Processing Batch 28: 100%|██████████| 5000/5000 [01:52<00:00, 44.42it/s]\n",
      "Processing Batch 29: 100%|██████████| 5000/5000 [01:53<00:00, 44.07it/s]\n",
      "Processing Batch 30: 100%|██████████| 5000/5000 [01:56<00:00, 43.07it/s]\n",
      "Processing Batch 31: 100%|██████████| 5000/5000 [01:55<00:00, 43.34it/s]\n",
      "Processing Batch 32: 100%|██████████| 5000/5000 [01:51<00:00, 44.94it/s]\n",
      "Processing Batch 33: 100%|██████████| 5000/5000 [01:55<00:00, 43.30it/s]\n",
      "Processing Batch 34: 100%|██████████| 5000/5000 [01:55<00:00, 43.31it/s]\n",
      "Processing Batch 35: 100%|██████████| 5000/5000 [01:56<00:00, 42.83it/s]\n",
      "Processing Batch 36: 100%|██████████| 5000/5000 [01:55<00:00, 43.40it/s]\n",
      "Processing Batch 37: 100%|██████████| 5000/5000 [01:54<00:00, 43.60it/s]\n",
      "Processing Batch 38: 100%|██████████| 5000/5000 [01:52<00:00, 44.39it/s]\n",
      "Processing Batch 39: 100%|██████████| 5000/5000 [01:55<00:00, 43.30it/s]\n",
      "Processing Batch 40: 100%|██████████| 5000/5000 [01:52<00:00, 44.55it/s]\n",
      "Processing Batch 41: 100%|██████████| 5000/5000 [01:53<00:00, 44.13it/s]\n",
      "Processing Batch 42: 100%|██████████| 5000/5000 [01:54<00:00, 43.60it/s]\n",
      "Processing Batch 43: 100%|██████████| 5000/5000 [02:00<00:00, 41.35it/s]\n",
      "Processing Batch 44: 100%|██████████| 5000/5000 [01:55<00:00, 43.30it/s]\n",
      "Processing Batch 45: 100%|██████████| 5000/5000 [01:53<00:00, 44.13it/s]\n",
      "Processing Batch 46: 100%|██████████| 5000/5000 [01:57<00:00, 42.50it/s]\n",
      "Processing Batch 47: 100%|██████████| 5000/5000 [01:59<00:00, 41.93it/s]\n",
      "Processing Batch 48: 100%|██████████| 5000/5000 [01:49<00:00, 45.67it/s]\n",
      "Processing Batch 49: 100%|██████████| 5000/5000 [01:52<00:00, 44.29it/s]\n",
      "Processing Batch 50: 100%|██████████| 5000/5000 [01:51<00:00, 44.79it/s]\n",
      "Processing Batch 51: 100%|██████████| 5000/5000 [01:53<00:00, 43.99it/s]\n",
      "Processing Batch 52: 100%|██████████| 5000/5000 [01:53<00:00, 44.12it/s]\n",
      "Processing Batch 53: 100%|██████████| 5000/5000 [02:01<00:00, 41.21it/s]\n",
      "Processing Batch 54: 100%|██████████| 5000/5000 [01:53<00:00, 44.24it/s]\n",
      "Processing Batch 55: 100%|██████████| 5000/5000 [02:01<00:00, 41.32it/s]\n",
      "Processing Batch 56: 100%|██████████| 5000/5000 [01:54<00:00, 43.61it/s]\n",
      "Processing Batch 57: 100%|██████████| 5000/5000 [01:51<00:00, 44.93it/s]\n",
      "Processing Batch 58: 100%|██████████| 5000/5000 [01:48<00:00, 46.27it/s]\n",
      "Processing Batch 59: 100%|██████████| 5000/5000 [01:49<00:00, 45.46it/s]\n",
      "Processing Batch 60: 100%|██████████| 5000/5000 [01:54<00:00, 43.57it/s]\n",
      "Processing Batch 61: 100%|██████████| 5000/5000 [01:48<00:00, 45.92it/s]\n",
      "Processing Batch 62: 100%|██████████| 5000/5000 [01:55<00:00, 43.36it/s]\n",
      "Processing Batch 63: 100%|██████████| 5000/5000 [01:50<00:00, 45.26it/s]\n",
      "Processing Batch 64: 100%|██████████| 5000/5000 [01:54<00:00, 43.84it/s]\n",
      "Processing Batch 65: 100%|██████████| 5000/5000 [01:52<00:00, 44.47it/s]\n",
      "Processing Batch 66: 100%|██████████| 5000/5000 [01:51<00:00, 44.80it/s]\n",
      "Processing Batch 67: 100%|██████████| 5000/5000 [01:52<00:00, 44.55it/s]\n",
      "Processing Batch 68: 100%|██████████| 5000/5000 [02:01<00:00, 41.22it/s]\n",
      "Processing Batch 69: 100%|██████████| 5000/5000 [01:52<00:00, 44.37it/s]\n",
      "Processing Batch 70: 100%|██████████| 5000/5000 [01:52<00:00, 44.47it/s]\n",
      "Processing Batch 71: 100%|██████████| 5000/5000 [01:55<00:00, 43.28it/s]\n",
      "Processing Batch 72: 100%|██████████| 5000/5000 [01:52<00:00, 44.30it/s]\n",
      "Processing Batch 73: 100%|██████████| 5000/5000 [01:53<00:00, 44.01it/s]\n",
      "Processing Batch 74: 100%|██████████| 5000/5000 [01:46<00:00, 46.74it/s]\n",
      "Processing Batch 75: 100%|██████████| 5000/5000 [02:01<00:00, 41.19it/s]\n",
      "Processing Batch 76: 100%|██████████| 5000/5000 [01:52<00:00, 44.62it/s]\n",
      "Processing Batch 77: 100%|██████████| 5000/5000 [01:46<00:00, 46.93it/s]\n",
      "Processing Batch 78: 100%|██████████| 5000/5000 [01:54<00:00, 43.59it/s]\n",
      "Processing Batch 79: 100%|██████████| 5000/5000 [01:54<00:00, 43.53it/s]\n",
      "Processing Batch 80: 100%|██████████| 5000/5000 [01:55<00:00, 43.42it/s]\n",
      "Processing Batch 81: 100%|██████████| 5000/5000 [01:52<00:00, 44.37it/s]\n",
      "Processing Batch 82: 100%|██████████| 5000/5000 [01:53<00:00, 44.01it/s]\n",
      "Processing Batch 83: 100%|██████████| 5000/5000 [01:48<00:00, 46.21it/s]\n",
      "Processing Batch 84: 100%|██████████| 5000/5000 [01:52<00:00, 44.31it/s]\n",
      "Processing Batch 85: 100%|██████████| 5000/5000 [01:53<00:00, 43.90it/s]\n",
      "Processing Batch 86: 100%|██████████| 5000/5000 [01:54<00:00, 43.74it/s]\n",
      "Processing Batch 87: 100%|██████████| 5000/5000 [01:56<00:00, 43.05it/s]\n",
      "Processing Batch 88: 100%|██████████| 5000/5000 [01:58<00:00, 42.29it/s]\n",
      "Processing Batch 89: 100%|██████████| 5000/5000 [01:57<00:00, 42.50it/s]\n",
      "Processing Batch 90: 100%|██████████| 5000/5000 [01:53<00:00, 44.04it/s]\n",
      "Processing Batch 91: 100%|██████████| 5000/5000 [01:51<00:00, 44.89it/s]\n",
      "Processing Batch 92: 100%|██████████| 5000/5000 [01:56<00:00, 43.07it/s]\n",
      "Processing Batch 93: 100%|██████████| 5000/5000 [01:49<00:00, 45.68it/s]\n",
      "Processing Batch 94: 100%|██████████| 5000/5000 [01:49<00:00, 45.80it/s]\n",
      "Processing Batch 95: 100%|██████████| 5000/5000 [01:54<00:00, 43.70it/s]\n",
      "Processing Batch 96: 100%|██████████| 5000/5000 [01:54<00:00, 43.55it/s]\n",
      "Processing Batch 97: 100%|██████████| 5000/5000 [01:46<00:00, 46.99it/s]\n",
      "Processing Batch 98: 100%|██████████| 5000/5000 [01:51<00:00, 44.95it/s]\n",
      "Processing Batch 99: 100%|██████████| 5000/5000 [01:59<00:00, 41.80it/s]\n",
      "Processing Batch 100: 100%|██████████| 1676/1676 [00:43<00:00, 38.77it/s]\n"
     ]
    }
   ],
   "source": [
    "db = Database(tokenize_fn=preprocess, docs=large_docs, n_processes=8, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6475080"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [(term, terminfo[0]) for term, terminfo in db.inverted_index.items()]\n",
    "\n",
    "len([t for t, _ in vocab if t.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('how to display how.close you are to.cell.tower', ['display'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_queries[1], preprocess(test_queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to display how close you are to cell tower\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"how.to.display how.close you are to.cell.tower\"\n",
    "clean_text = re.sub(r'[^a-zA-Z]+', ' ', text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['display', 'close', 'cell', 'tower']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/preprocessing_large_database.pkl\", \"wb\") as file:\n",
    "    pickle.dump(db, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6475081"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/preprocessing_large_database.pkl\", \"rb\") as file:\n",
    "    db = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries for MAP@K=3:  48%|████▊     | 2494/5193 [03:46<06:29,  6.92it/s]"
     ]
    }
   ],
   "source": [
    "# Example usage for k = 3 and k = 10\n",
    "k_values = [3, 10]\n",
    "\n",
    "for k in k_values:\n",
    "    map_k = map_at_k(large_queries, large_query_results,\"db.pickle\", query_database, k)\n",
    "    mar_k = mar_at_k(large_queries, large_query_results, \"db.pickle\", query_database, k)\n",
    "    print(f\"MAP@{k}: {map_k}\")\n",
    "    print(f\"MAR@{k}: {mar_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_db = PositionalDatabase(tokenize_fn=preprocess, docs=large_docs, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_db.merge_partial_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/preprocessing_large_pos_database.pkl\", \"wb\") as file:\n",
    "    pickle.dump(pos_db, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/preprocessing_large_pos_database.pkl\", \"rb\") as file:\n",
    "    pos_db = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for k = 3 and k = 10\n",
    "k_values = [3, 10]\n",
    "\n",
    "query_function = partial(pos_query_database, q_fraction=1, boost_factor=1)\n",
    "\n",
    "for k in k_values:\n",
    "    map_k = map_at_k(large_queries, large_query_results, pos_db, query_function, k)\n",
    "    mar_k = mar_at_k(large_queries, large_query_results, pos_db, query_function, k)\n",
    "    print(f\"MAP@{k}: {map_k}\")\n",
    "    print(f\"MAR@{k}: {mar_k}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
